# My Kernels Collection

I write kernels here

- Check out [chill attention](https://github.com/alexdremov/chill-attention) â€” configurable sparse attention kernel! 
- [Fused Self Attention (forward)](https://github.com/alexdremov/kernels/blob/main/src/self_attention/kernel.py)
  - check out the giude to understanding it: [https://alexdremov.me/understanding-flash-attention-writing-the-algorithm-from-scratch-in-triton/](https://alexdremov.me/understanding-flash-attention-writing-the-algorithm-from-scratch-in-triton?utm_source=github_kernels)
- [Streaming Attention (forward, backward, pt2 compliant)](https://github.com/alexdremov/kernels/blob/main/src/streaming_attention/kernel.py)
  - [detailed description in the docstring](https://github.com/alexdremov/kernels/blob/9d2fde773ee9bfc8c838fb6cad741323b7e02afb/src/streaming_attention/kernel.py#L1489)
  - TBD: guide to why it is cool (nudge me if interested)
